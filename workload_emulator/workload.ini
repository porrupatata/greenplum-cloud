[general_settings]
##mode: either production or development. It refers to the environment we are using.
mode='production'
#mode='development'
##database_type refers to the DB connection class we'll be using. It should be 'postgres' for GP, aurora, redshift...
database_type = 'postgres'
query_folder= 'queries_gp'

[production_settings]
sqlite_db='workload.db'
scheduler_frequency=60.0
query_frequency_file='workload_frequency_dev-evalqueries.csv'
run_queries='yes'

[development_settings]
sqlite_db='workload_test.db'
scheduler_frequency=1.0
query_frequency_file='workload_frequency.csv'
run_queries='no'
#run_queries='yes'

[test_settings]
platform='greenplum_cloud'
queues='d1f1','d1f1v1','d1f1v2','d1f1v3' 
#queues='d1f1'
#queues='d1f1','d1f2','d1f4'
#queues='d1f1','d2f1','d3f1','d1f2','d2f2','d3f2','d1f4','d2f4','d3f4','d1f8','d2f8','d3f8','d1f16','d2f16','d3f16'

details= {
         "instance_type" : "r5.xlarge master and r5.4xlarge",
         "version": "9.6",
	 	#"work_mem":"IM/400000 -> 0.65GB",
         #"shared_buffers":"IM/21844-> 95GB",
         "note" : "greenplum on cloud 20200113-"
         }


[query_queue]
## duration is in hours. The duration of the queue. 2 means, there will be a queue of 2 hours, and we'll be sending queries during those 2 hours.
## frequency is a multiplier for the frequency of the real query frequency. If one query is executed twice every hour and freq is set to 4, the query will be executed 2x4=8 times per hour
#queue_creation= {
#	"d1f1": { "duration" :1 , "freq":1}
#	}


queue_creation= {
	"d1f1": { "duration" :1 , "freq":1},
	"d2f1": { "duration" :2 , "freq":1},
	"d3f1": { "duration" :3 , "freq":1},
	"d1f2": { "duration" :1 , "freq":2},
	"d2f2": { "duration" :2 , "freq":2},
	"d3f2": { "duration" :3 , "freq":2},
	"d1f4": { "duration" :1 , "freq":4},
	"d2f4": { "duration" :2 , "freq":4},
	"d3f4": { "duration" :3 , "freq":4},
	"d1f8": { "duration" :1 , "freq":8},
	"d2f8": { "duration" :2 , "freq":8},
	"d3f8": { "duration" :3 , "freq":8},
	"d1f16": { "duration" :1 , "freq":16},
	"d2f16": { "duration" :2 , "freq":16},
	"d3f16": { "duration" :3 , "freq":16},
	"d1f1v1": { "duration" :1 , "freq":1},
	"d1f2v1": { "duration" :1 , "freq":2},
	"d1f4v1": { "duration" :1 , "freq":4},
	"d1f1v2": { "duration" :1 , "freq":1},
	"d1f2v2": { "duration" :1 , "freq":2},
	"d1f4v2": { "duration" :1 , "freq":4},
	"d1f1v3": { "duration" :1 , "freq":1},
	"d1f2v3": { "duration" :1 , "freq":2},
	"d1f4v3": { "duration" :1 , "freq":4}
	}
create_test_queue='t'
backup_restore_query_files='backup_1'


[postgres_connection]

# Greenplum on Cloud Connection Settings
host = 'x.x.x.x'
database = 'xxxx'
user = 'xxxxx'
password = 'xxxxxx'
port = 5432

[sqlite_connection]
database = 'workload_report_data'

[data_output]
csv_filename = 'workload.csv'

[query_filters]
#use your own filters
country_id = 10::INT,14::INT,21::INT,150::INT
start_date= '2017-10-01','2017-12-20','2018-01-01','2018-02-15','2018-04-23','2018-06-01','2018-09-03','2019-10-15','2019-01-01'
end_date = '2019-10-01','2020-01-01'
category_id = 3,12,13,16,17,21,558,48,53,54,55,56,57,59,64,577,579,70,76,89,617,128,646,135,647,662,666,671,177,692,693,699,700,701,702,717,723,725,219,732,740,743,758,759,770,771,772,773,774,775,776,777,778,779,780,781,786,791,798,810,811,812,813,817,818,819,820,825,324,842,334,335,849,342,345,857,860,862,864,868,869,871,872,875,876,881,370,883,372,371,897,385,387,388,6128,72693
datasource_id = 85,84,127
customer_country_id = 258,132,5,141,14,269,21,150,30,160,171,172,177,56,57,58,192,67,196,198,70,72,73,204,205,80,82,83,84,217,224,98,103,105,117,122,123,124,255
id = 1,2,3,4,5

[queries]
sql_statements = [
					"select * from pg_aggregate",
					"select * from pg_am",
					"select * from pg_rewrite"]
